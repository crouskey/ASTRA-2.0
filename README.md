Step #
Topic
Subsystems
Description
Completion
Priority
Dependencies
1
Project Setup & Architecture Design
Paperspace VM, Supabase, Repo
Initialize the environment: Spin up a Paperspace machine (using Gradient or a VM) and create a project repository (Git). Decide on the tech stack: use Node.js/Next.js for the server (aligns well with the React frontend and rich JS ecosystem ) and integrate Supabase (PostgreSQL) as the cloud database. Architectural decisions: Follow the memory design â€“ store large raw files and long-term data on the Paperspace file system, and use Supabase for structured short-term data and metadata. Outline core components (voice, chat, code engine, etc.) and how they interact. Plan integration points (e.g., API routes or WebSocket for real-time features). Ensure you have API keys and credentials ready for OpenAI (ChatGPT), ElevenLabs, Google APIs, etc. Sketch the schema and high-level module structure now to guide subsequent steps.
[ ]
High
N/A
2
Install Dependencies & Configurations
OpenAI API, ElevenLabs API, Google APIs, Supabase SDK
Set up the codebase: Initialize the Node/Next.js project and install required libraries. This includes the OpenAI SDK (for ChatGPT), ElevenLabs SDK or HTTP client (for voice), Supabase client library (for database access), Google API client (googleapis for Gmail/Calendar), and any others (e.g., node-cron for scheduling, nodemailer or Gmail API for email, node-pty for shell access). Also install UI dependencies (React, Tailwind CSS, ShadCN UI components, Framer Motion) via Lovable.devâ€™s recommended setup. Configuration: Add environment variables for all API keys (OpenAI key, ElevenLabs key, Google OAuth creds, Supabase URL/key, etc.) and create a config file or use environment to store them securely. Verify connectivity: e.g., test a simple Supabase query and an OpenAI API call. Ensure the Paperspace machine has necessary tools (Node.js, perhaps Python if needed for parsing, Git). This step lays the groundwork for all upcoming development.
[ ]
High
Step 1
3
Database Schema Design (Supabase)
Supabase DB (Postgres), pgvector
Design the data model in Supabase: Create tables to support conversations, memory, files, logs, and users. For example, a users table (if using Supabase Auth, it provides user IDs), a messages table (chat history with columns: user_id, role, content, timestamp), a knowledge table (for knowledge graph nodes/edges or extracted facts), a files table (file metadata: owner, file_path on Paperspace, upload date, summary, etc.), an embeddings table (for vector representations of file content or knowledge, using pgvector), an emotion_log table (session mood metrics), and a daily_summaries table (storing each day's email summary, commit log, etc.). Ensure each table has a user/instance identifier to segregate data per user. Enable multi-user security: Turn on Row-Level Security and write policies so that each user can only access their own rows (Supabase's RLS will enforce queries like user_id = auth.uid() automatically ). This schema must accommodate future multi-tenant use, meaning other deployments can use the same structure without data collision . Also include fields for permissions/sharing (for instance, a file could have a flag or shared-with list for cross-instance sharing).
[ ]
High
Steps 1-2
4
Conversational Memory (ChatGPT Int.)
OpenAI GPT-4, Memory Store (Supabase)
Integrate ChatGPT for dialogue: Set up an API call to OpenAI (GPT-4 or GPT-3.5) to handle the assistantâ€™s responses. Define Astraâ€™s base personality via a system prompt (e.g. â€œYou are Astra, a helpful AI assistantâ€¦â€) so that the assistantâ€™s tone is consistent. Implement conversation memory: Maintain a running list of recent messages (user and assistant) to include in each API call for context. Use Supabase to persist this history â€“ on each user message, and save it (and the assistant response) to the messages table. For long sessions, implement a strategy to avoid hitting token limits: e.g., summarize older messages and store those summaries in the database as long-term memory, while keeping recent dialogue verbatim in the prompt. This mirrors how ChatGPT uses conversation history to personalize replies. The assistant can retrieve past facts from Supabase (or the knowledge graph) to stay consistent. Ensure the design allows real-time updates: after each assistant response, also update any relevant â€œmemoryâ€ (for instance, store new facts in the knowledge graph in Step 6). By the end of this step, sending a text query to Astra should return a coherent, persona-aligned answer that remembers prior context (to the extent of the chosen memory window and stored summaries).
[ ]
High
Steps 2-3
5
Voice I/O (Speech & Synthesis)
ElevenLabs STT/TTS, Audio Interface
Voice Input: Integrate ElevenLabs Speech-to-Text API to transcribe user speech. When the user holds a mic button in the UI (to be built), capture audio (e.g., Web MediaRecorder in frontend, or a WAV upload) and send it to the backend. Use ElevenLabs' Scribe model via their API to get text transcripts. Process this text as a user message into the chat system. Voice Output: Take the assistantâ€™s text reply and send it to ElevenLabs Text-to-Speech API. Choose a voice (perhaps a custom ethereal voice preset) and synthesize the reply, obtaining an audio file/stream. Send this audio back to the front end to play for the user. Handle streaming if possible (ElevenLabs may allow low-latency or stream API). Ensure real-time feel: The goal is a seamless voice conversation, so minimize latency (perhaps send the userâ€™s audio as soon as recorded, and play the generated speech immediately upon receipt). With this step, Astra can hear and speak: you can talk to it via microphone and it responds with realistic speech.
[ ]
High
Steps 2, 4
6
Knowledge Graph & Memory Graph Logic
Knowledge Graph (nodes/edges), Timeline store (Supabase)
Implement structured knowledge storage: Whenever Astra learns a new fact (from conversation or an uploaded file), represent it in a knowledge graph. Simplify implementation by using Supabase tables to store graph nodes (entities/concepts) and edges (relationships) â€“ e.g., a knowledge table with columns: id, type (person, concept, etc.), content, and a relations table with source_id, target_id, relation_type. Populate this as the system ingests information: for instance, if a user mentions â€œAlice is Bobâ€™s managerâ€, create nodes for â€œAliceâ€ and â€œBobâ€ (if not exist) and a relation â€œmanager_of(Alice, Bob)â€. Utilize GPT to extract such entities and relations from texts, as suggested by personal knowledge management practices . Memory graph and timeline: Also log key events along a timeline â€“ e.g., each conversation session, file upload, email summary creation, etc., with timestamps. These can be stored in a events or logs table (timestamp, description, tags, related entities). The combination forms a â€œmemory graphâ€: structured memory linking what Astra has learned and when. Although a full graph database isnâ€™t feasible to set up in a day, using Postgres relations allows later visualization and query (for example, find connections between topics, or see chronology of a certain topicâ€™s mentions). By end of this step, Astra has an internal structured memory: it can record knowledge in a graph form (enabling future reasoning queries like â€œhow does X relate to Y?â€) and maintain a timeline of important events in its operation.
[ ]
High
Steps 3-4
7
File Ingestion & Learning Pipeline
File Upload handler, Parsers, Embeddings (pgvector)
File upload handling: Enable Astra to accept files of all types and extract their content. Implement an upload endpoint (or use Supabase Storage if convenient) for PDFs, Word docs, spreadsheets, audio/video files, etc. For text-based files (PDF, DOCX, TXT, PPTX), use parsing libraries or services â€“ e.g., textract in Python/Node which can handle many formats generically . For PDFs specifically, you might use PDF.js or PDFPlumber; for Word, use a docx parser; for spreadsheets, csv or Excel parsers. For audio/video, perform speech-to-text (e.g., use ElevenLabs or Whisper on audio tracks) to get transcripts. Metadata & content extraction: Once the raw text is extracted, save the fileâ€™s metadata (filename, type, upload time, owner) in the files table. Automatic learning: Summarize and embed the content so Astra can learn from it. Use OpenAIâ€™s embedding API to generate vector embeddings of the fileâ€™s text content , and store this vector in the embeddings table (with reference to the file or knowledge entries). Also store a short summary or outline of the file in the database. Connect this with the knowledge graph: e.g., update the graph with any entities in the file (using GPT to extract them and linking to existing nodes if any). Daily operation usage: Ensure these file-derived knowledge pieces are integrated into Astraâ€™s responses â€“ i.e., implement a retrieval step before GPT responds: query the embeddings table for relevant content (using vector similarity via pgvector) and supply the most relevant file snippets to GPT as context. This way, Astra â€œlearnsâ€ from all uploaded files and can answer questions about them. Finally, log the file ingestion in the timeline (for transparency). After this step, you can drop a PDF or video into Astra, and it will assimilate that information into its knowledge base and remember it going forward.
[ ]
High
Steps 3, 4
8
Self-Evolving Code Engine â€“ Design
GPT Code Assistant, Git (version control)
Plan the self-modifying code workflow: Architect how Astra will improve its own code. The idea is to let the AI suggest code changes, apply them safely, and roll back if needed. Define triggers: e.g., a user can ask Astra to refine its code or Astra can identify an improvement or bug (perhaps through error logs or performance metrics) and initiate a self-update. Environment setup for code edits: Ensure the running code is accessible (since itâ€™s a live system, it could edit in-memory or on-disk files). Using Git as the single source of truth is wise â€“ the current code is checked out in the Paperspace environment. Using GPT for code suggestions: When triggered, have Astra gather context (possibly the relevant file content or error trace) and prompt OpenAI (GPT-4) with instructions to modify the code. For example: â€œHere is function X, itâ€™s not optimal, please suggest improvements.â€ GPT returns a diff or new code. Prepare for changes: Before applying, take a snapshot of the current version (e.g., stash the git diff or copy the file) to allow rollback . Then apply the changes â€“ e.g., write the new code to the file. Use git diff to obtain a human-readable diff of changes, and log this in a â€œcode_changesâ€ log (and to Supabase for record, maybe linking to the daily summary). The system should attach GPTâ€™s reasoning for the change (ask GPT to comment its rationale). Version control: Commit the change to Git with a message (possibly AI-generated summary). By structuring the workflow with context gathering, GPT generation, and version control, we set the stage for iterative self-improvement while maintaining a safety net (the snapshot for rollback).
[ ]
High
Steps 1, 4
9
Self-Evolving Code Engine â€“ Execution
Sandbox Testing, Validation, Rollback
Validate and test changes: After applying a self-generated code change, run a test or at least reload that part of the system. For Node, this could mean running the file in a child process (for example, if itâ€™s a function, import and execute a test case; if itâ€™s server code, run a dry-run). If you have a test suite, automate running it now. Leverage the concept of â€œself-healingâ€ code: if the new code fails tests or causes an error, automatically revert to the previous snapshot . Implement the rollback by restoring the saved file or doing a git revert. Log the failure and rollback action to the Supabase logs (so it can be part of the reflective summary). Reasoning and iteration: If a change fails, Astra can ask GPT for a corrected approach (this could loop a few times with caution not to infinite loop â€“ set a limit of attempts ). Provide GPT the error output to refine the suggestion. Each iteration, preserve the ability to rollback. User oversight: Design the system such that final code changes ideally need user confirmation (perhaps via the UI in step 17). However, for speed, Astra might auto-apply trivial fixes. All code modifications and their outcomes should be stored (in a code_log table with fields: timestamp, change summary, success/fail, etc.). By the end of this step, Astra has the capability to modify its own codebase in a controlled manner â€“ proposing edits, testing them in a sandbox, and rolling back if they donâ€™t pass validations. This provides a foundation for an autonomous improvement cycle , making the system somewhat self-evolving while maintaining reliability.
[ ]
High
Steps 8
10
Animated Avatar & Emotion Rendering
Avatar Canvas, Sentiment Analysis
Create Astraâ€™s visual presence: Design an â€œethereal pixel faceâ€ avatar that reflects emotion. For simplicity and style, use a small canvas orÂ  in the frontend to draw a pixelated face. Prepare a few base facial states (neutral, happy, sad, surprised, etc.), possibly as pixel art images or procedurally drawn shapes (eyes, mouth) that can move. Use Framer Motion for smooth transitions or subtle animations (e.g., idle blinking, or a glow effect around the avatar changing color with mood). Emotion input: Feed the avatar an emotion value in real-time. This comes from sentiment analysis on the conversation and voice tone. Implement a sentiment analysis on Astraâ€™s side: after each user input or each assistant response, analyze the text for sentiment. For text, you can use a simple Node sentiment library (e.g. the â€œsentimentâ€ npm which uses AFINN wordlists ) to get a positive/negative score. For voice tone (optional), measure speech attributes (pitch, speed â€“ perhaps use an energy or spectral analysis if feasible) to detect excitement or calmness. Map these inputs to an emotion state for the session (e.g., very positive sentiment => happy face; negative => sad; neutral => neutral; high intensity voice => excited/surprised expression). Procedural rendering: Write a function that updates the avatar drawing based on an emotion enum. For example, if mood becomes â€œhappyâ€, draw the face with a smile and maybe a bright color; if â€œfocusedâ€, maybe a neutral mouth and narrowed eyes. Ensure this update happens whenever mood changes. Integrate with chat: Place the avatar near the chat interface (e.g., a corner of the chat window or as a subtle background element) and have it update continuously. By completing this, Astra now has a â€œfaceâ€ that gives users visual feedback, making interactions feel more alive and emotionally attuned to the conversation.
[ ]
Medium
Steps 4, 5, 6
11
Email & Calendar Integration
Gmail API, Google Calendar API
Link Gmail: Integrate Astra with the userâ€™s Gmail account (in this case, craigrouskey@gmail.com). Use Googleâ€™s API (via the googleapis Node library) to authorize access to Gmail and Calendar. Set up OAuth credentials and perform the consent flow to get a refresh token for offline access. Email reading: Implement a module to regularly fetch recent emails (e.g., the past day or specific label/inbox) via Gmail API and summarize them. Astra can use GPT to summarize email content or just list subjects. Also allow Astra to search emails or read specific ones on command. Email sending: Enable Astra to draft or send emails â€“ e.g., the daily 6am summary (Step 12) or if the user asks â€œAstra, email John that Iâ€™ll be late,â€ Astra can use the Gmail API to send that message. Calendar integration: Similarly, use Google Calendar API to fetch upcoming events (for daily briefings) and allow Astra to create events (if user says â€œschedule a meeting tomorrow at 3pmâ€). The Google API client makes it straightforward (e.g., google.calendar({version:'v3'}).events.list(...) to list events ). Data storage: Store key email meta (like number of unread, summary of important ones) and calendar events for the day in Supabase (this goes into daily_summaries or a separate table). After this step, Astra can act as a personal assistant managing emails and calendar: it can read your schedule, add events, retrieve emails, and prepare information to include in summaries or answer questions like â€œDid I get any emails about Project X?â€.
[ ]
High
Steps 2-4
12
Daily 6am Summary Cron Job
Scheduler (cron), Email module, Summary generator
Automate daily reflection email: Set up a scheduled job to run at 6am PST every day. On Paperspace, you can use a simple cron (since itâ€™s a full server) by adding a cron entry that hits an endpoint or runs a script, or use a Node scheduler like node-cron to schedule a function at 6am. This job will compile the â€œreflective summaryâ€ email for the previous dayâ€™s activity. Summary content: Gather data from the past day: chat highlights (maybe summarize the last session or any important user instructions), new knowledge acquired (files uploaded or facts learned), mood trend (was the assistant mostly positive or negative?), notable log events (commands run, errors), and a brief of emails and calendar items for the day. Use the data stored in Supabase: e.g., query yesterdayâ€™s messages or actions and have GPT produce a coherent narrative. Also include the commit log from the self-evolving code (if any code changes happened, list them) and any tasks completed. Email dispatch: Once compiled, send this summary to the userâ€™s email via Gmail API (from Step 11). The email should be well-formatted (could be HTML with lists for each category of update). Log the summary: Save the content of the summary in the daily_summaries table with a date. This allows Astra to reference what it told the user (and not repeat excessively) and also serves as a journal. By implementing this, each morning the user will wake up to a concise report from Astra about the systemâ€™s state and activities, fulfilling the cron requirement. Test this manually by triggering the function once to ensure the email sends correctly (before relying on the timed schedule).
[ ]
High
Steps 3, 11
13
Mood Tracking & Analytics
Sentiment Analyzer, Supabase (emotions)
Track emotional state per session: Expand the sentiment analysis from Step 10 to quantify each user session or day. Define what constitutes a â€œsessionâ€ (e.g., a continuous chat until a long idle, or each day as one session). Calculate metrics like average sentiment score, variance, and predominant emotion. For example, a session where the user was frustrated might have mostly negative sentiment from the userâ€™s messages; Astra could classify overall mood as â€œfrustratingâ€. Visualization data: For each session or day, store an entry in the emotion_log (or daily_summaries) with an overall mood label and perhaps a numerical score. E.g., {date: 2025-04-11, user_id: X, mood: â€œPositiveâ€, sentiment_avg: 0.7}. Use a simple scale from very negative to very positive. This can be obtained by aggregating message sentiments. Real-time mood updates: Also track mood throughout a conversation â€“ e.g., maintain a rolling sentiment score updated after each turn. This was used in Step 10 for the avatar; here we ensure itâ€™s logged. Supabase can store a time series of sentiment if desired (for a graph of mood over time). Privacy and nuance: Note that emotion detection is approximate; keep the analysis respectful and use it to improve responses (Astra might say â€œI sense you sound upsetâ€ if negative sentiment is high, to address user emotion). By the end, the system has an emotional memory: it knows how interactions tend to go and can display and recall emotional trends. This data will feed into the UI dashboards (user can see a timeline of their mood or Astraâ€™s mood).
[ ]
Medium
Step 10
14
Multi-User & Instance Segregation
Supabase Auth & RLS, Instance Config
Support multiple users and deployments: Expand the configuration so Astra isnâ€™t hardcoded to one user. If using Supabase Auth, integrate the frontend with sign-up/login to get a user ID token. Use that user_id in all Supabase queries so each userâ€™s data stays isolated by the RLS policies from Step 3 (ensuring each tenant only accesses their own rows ). Test that by creating a second dummy user and verifying they canâ€™t query data that isnâ€™t theirs. Permission layers: Implement role-based checks if needed (e.g., an â€œadminâ€ user may have access to some global settings or shared files). Deploy on other Paperspace accounts: Document or script the deployment process so someone else can deploy Astra. This might include a setup script that initializes the Supabase schema (if they use a different Supabase instance) or instructs them to use the same central Supabase for a network of Astra instances. Linking instances for file sharing: Design a method for instances to share data. A simple approach: designate certain knowledge or files as â€œpublicâ€ or shareable, and if multiple Astra instances connect to the same Supabase (with appropriate read rights), they can pull those shared records. For example, user A can mark a file as shared with user B; this creates a record that user Bâ€™s instance can see (via a group or shared table). Alternatively, use an API where Astra A sends the file content to Astra B (but thatâ€™s complex for now). Given time constraints, note this as an extension â€“ but ensure the schema can handle a â€œsharedâ€ flag or table. After this step, Astraâ€™s backend is truly multi-tenant: ready for future deployments where each userâ€™s memories and files are kept separate and secure, and laying groundwork for optional cross-instance collaboration.
[ ]
Medium
Steps 3, 11
15
Frontend Setup (Lovable.dev + React)
Next.js, Tailwind, ShadCN UI, Auth
Initialize the frontend project: If not already part of the Next.js app, use Lovable.dev to set up a React/Next.js frontend with Tailwind CSS and ShadCN UI (Radix-based components). Configure the styling and theme (perhaps a dark theme for the â€œetherealâ€ vibe). Layout: Create the main application page structure: a chat interface page, a dashboard page, maybe a login page if multi-user. Install ShadCN UI components via their CLI or by copying components as needed (for example, modal, button, input field components). Set up Tailwind config (include Framer Motion and any needed plugins). State management: If the app is complex, introduce a state management solution (Zustand or Redux) to handle global states like current user, current conversation, etc., or use React Context. Auth: Integrate Supabase Auth or a custom login if multi-user â€“ possibly use Supabaseâ€™s Auth UI or Magic Link for simplicity, or skip if only one user for now (in which case, lock the app to that userâ€™s data). Test connectivity: Make a test query from the frontend to an API route or directly to Supabase (if using Supabase client in frontend for some data). Also, ensure the frontend can reach the backend (if using Next.js API routes, they are in the same project; if using a separate server, configure CORS and base URLs). By the end of this, the project is ready to build UI features: you have a running React app served (possibly on Paperspace or locally) with design systems in place, awaiting the implementation of specific interface elements.
[ ]
High
Steps 1-2
16
Chat Interface (UI + Voice + Avatar)
React Chat UI, Mic input, Audio output, Avatar component
Build the chat UI: On the main page, create a chat window panel. Use a combination of custom code and ShadCN components (like Card or ScrollArea) to display the conversation messages. Each message can be a styled bubble â€“ user messages on right, Astraâ€™s on left with a distinct color. Implement scrolling and perhaps timestamp or minor details for each message. Input bar: At the bottom, have a single-line text input for the userâ€™s query and a send button. Next to it, add a microphone button. When clicked, start recording audio (use the Browser MediaRecorder API). Provide visual feedback (like a recording dot). On release/stop, send the audio blob to the backend (an API route) for transcription (Step 5). While audio is being transcribed or the AI is formulating a reply, show a loading indicator (maybe three dots animation). Voice output: When the backend returns a response, it will include text and possibly a URL or blob of the spoken audio. Append the assistantâ€™s text to the chat window as a message. Then play the audio â€“ create an HTML5 audio element or use Web Audio API to stream it. Ensure the user can hear Astraâ€™s reply. Avatar integration: Place the animated pixel face (from Step 10) near the chat â€“ for example, at the top of the chat window or as a floating element â€“ and update it with the current emotion state (you can tie this to each message: e.g., if userâ€™s last message was angry, maybe the avatar looks concerned). Use Framer Motion to animate transitions (the avatar might fade in with each new state or gently pulse with the dialogue flow). Chat-to-command execution: Consider that if the user issues a special command (like a prefix â€œ/shell do Xâ€), the system should interpret it as a server command (Step 11, terminal). Implement a simple check on the input component for such prefixes to route the request accordingly (e.g., call a different API endpoint to run a command instead of ChatGPT). By completing this step, the front-end provides an interactive chat experience: type or talk to Astra and see its face and hear its voice responding in real time.
[ ]
High
Steps 5, 10, 15
17
Self-Evolution UI (Memory & Code Ctrl)
React (Modal, Buttons), Diff Viewer
Memory management UI: Provide controls for the user to influence Astraâ€™s memory. For example, a button to â€œSummarize Memory Nowâ€ â€“ when clicked, it triggers the backend to compress older messages into a summary (useful if the conversation is long). Another could be â€œClear Memoryâ€ â€“ wiping the conversation (in case the user wants to start fresh). Also, possibly list some of the â€œSaved factsâ€ from the knowledge graph so the user knows what Astra has learned (for transparency). These can be shown in a sidebar or modal. For instance, a â€œKnowledgeâ€ sidebar that enumerates key facts/entities Astra knows so far. Code review UI: When the self-evolving code engine (Steps 8-9) generates a code diff, present it to the user for confirmation (unless fully automated). Implement a modal popup that shows the code changes: e.g., a two-column diff view. You can use a diff highlighting library or simply <pre> tags with added/removed lines colored (green/red). Show the AIâ€™s reasoning (which we logged) alongside â€“ e.g., â€œReason: Improved performance of X by caching.â€ Then provide â€œAcceptâ€ and â€œRollbackâ€ buttons. Accept would finalize the change (perhaps just close the modal since code is already applied, or if we held off on applying, then commit now), and Rollback would revert the change via the backend API. Controls for self-improvement: Also include a button â€œSelf-Improveâ€ to manually trigger the self-evolution process â€“ if the user clicks it, the backend will run the code analysis prompt and return a proposal (which then pops up as above). These UI elements ensure the user can oversee and influence Astraâ€™s self-modifications and memory. After this step, the user interface not only shows chat, but also gives insight and control over Astraâ€™s internals â€“ a key part of trusting an AI that modifies itself.
[ ]
High
Steps 4, 8-9, 15
18
Dashboard Metrics & Graphs (UI)
Charts (emotion graph), Knowledge Graph Vis, Logs view
Create the dashboard page: This page will present visualizations of Astraâ€™s state and activities. Divide it into sections: e.g., â€œEmotional Trendâ€, â€œKnowledge Graphâ€, â€œSystem Activityâ€. Memory/Emotion state visualization: Implement a chart showing mood over time. For example, a line chart of sentiment score across the last few sessions or days. Use a library like Chart.js or a lightweight React chart component to plot data from the emotion_log (Step 13). Alternatively, show a gauge or pie for the current sessionâ€™s mood distribution. Knowledge graph visualization: Use a JS graph library (like D3.js or Sigma.js) to display the knowledge graph nodes and edges . Query the knowledge and relations tables for a subset of the graph (maybe the most relevant 20 nodes to current conversation) and render an interactive network. Each node could be clickable to show details (facts associated, when learned). The graph gives a snapshot of what Astra â€œknowsâ€ and how facts connect. Timeline of events: Show a scrollable timeline of notable events (file uploads, emails sent, code changes, etc.). This could be a simple list grouped by day with timestamps and icons (e.g., a file icon for file ingest, an email icon for summary sent). ShadCNâ€™s Card or Accordion components can format this nicely per day. Server metrics: If feasible, display current server stats (CPU, memory usage of the Paperspace VM) to monitor performance. You can get these from a small API (Nodeâ€™s os module for CPU load, memory) and update periodically. Interactivity and polish: Use Framer Motion for animations when switching to the dashboard (for example, fade in charts or animate nodes in the graph). Ensure the UI remains responsive (Tailwind will help here) so the dashboard looks good on various screen sizes. By the end of this step, the user has a rich dashboard to peek under Astraâ€™s hood â€“ from how itâ€™s feeling to what it knows and what itâ€™s been up to. This satisfies the requirement for graphical representations of memory, emotions, knowledge, and activity.
[ ]
High
Steps 6, 7, 13, 15
19
In-Browser Terminal Access (UI/Back)
Node PTY, WebSocket, Xterm.js
Backend shell access: Implement a secure route to execute shell commands on the Paperspace server. Use the node-pty library to spawn a pseudo-terminal (bash shell) process on the server. Create a WebSocket endpoint or similar that the frontend can connect to. When the user types in the terminal UI, send that input to the PTY and stream back the output. Limit permissions (maybe run as a restricted user or within the project directory) to prevent destructive commands, and require a special permission (only the owner user can access this feature). Log all commands run to the Supabase logs for auditing. Frontend terminal UI: Use xterm.js in the React frontend to create a terminal emulator component . Xterm.js will render a live terminal in the browser that looks and behaves like a real console. Establish a WebSocket connection from the frontend to the backend when the terminal component mounts. Configure it to send keystrokes and display output. Integration into app: Provide a â€œTerminalâ€ tab or button in the interface (perhaps in the Dashboard or as a pop-up window). Only show it to authorized users. The terminal can be used to manually inspect the server, run git commands, edit files with vi, etc., essentially giving the developer full control remotely. Use cases and safety: This is a powerful feature â€“ it allows quick debugging or running commands that Astra might not cover with its UI. Emphasize caution: the user should know what theyâ€™re doing (maybe add a confirmation before opening terminal). By finishing this, the Astra system includes an embedded web terminal, meeting the requirement for direct server manipulation through the interface. It leverages proven tools (xterm.js and node-pty) to provide a robust experience similar to an SSH session in the browser.
[ ]
Medium
Step 1 (env), possibly 14 (auth)
20
GitHub CI/CD Integration
GitHub repo, GitHub Actions, Paperspace Deploy
Set up version control: If not already, push the code to a GitHub repository. Organize the repository (maybe a monorepo with backend and frontend if separate, or just one Next.js app). Commit all changes and ensure that secrets (API keys) are not in repo. Continuous Deployment: Use GitHub Actions to automate deployment to Paperspace. Paperspace offers a deploy Action that can take a container image or project ID and deploy updates . Create a workflow YAML in the repo that triggers on pushes to main. The steps could be: build the Docker image (if using container) or zip the project, then use the Paperspace Deploy Action with the Paperspace API key to push it. Alternatively, if not containerizing, set up a simple script to ssh or pull latest code on the VM (less ideal). The CI pipeline should also run any tests if available (for example, you might have basic tests for the code engine logic or parsing functions). Continuous Integration (code history tracking): Every commit is already tracked in Git, but we also want commit logs in Supabase. Implement a small job (could be an Action or part of the 6am cron) that uses the GitHub API or git log to retrieve yesterdayâ€™s commits and store them in the daily_summaries table. This ensures the reflective email can include â€œCode changes: â€¦â€ each day. Monitoring deployment: Once CI is set, do a test push and verify that the Paperspace instance updates (the Action should output success). With this step, any code changes (including those Astra might make to itself!) are recorded and automatically deployed, closing the loop for continuous improvement. The GitHub integration not only provides version history (which aids the rollback mechanism) but also makes it easy to deploy Astra to other machines or revert to previous versions via Git.
[ ]
Medium
Steps 8-9 (for tests), 1 (for Paperspace project)
21
Testing, Tuning, and Final Integration
All subsystems (end-to-end)
End-to-end testing: Now that all major features are in place, thoroughly test each component and their interaction. Simulate real usage: Start a conversation via voice, ensure transcription is accurate and response is appropriate (tweak the system prompt or parameters if the personality isnâ€™t coming through). Upload various file types and ask Astra questions about them to verify the content extraction and retrieval (if answers are incorrect, adjust the embedding search or increase context given to GPT). Test email and calendar by asking â€œWhatâ€™s on my schedule today?â€ or verifying the 6am summary arrives with correct info. Use the terminal to run a harmless command (like ls or git status) and see that output returns. Fix bugs and tune: You might encounter issues (e.g., audio too slow, or knowledge graph queries not used by GPT). Adjust accordingly: maybe pre-fetch relevant knowledge graph info and include it in the prompt for GPT, or optimize audio encoding to reduce size, etc. Ensure the self-evolution loop doesnâ€™t run wild â€“ perhaps test it with a contrived improvement (ask it to add a comment somewhere) and see that it asks for confirmation and can rollback if you simulate a failure. Performance checks: Monitor resource usage (the dashboardâ€™s metrics can help) during heavy use. If memory or CPU is an issue, consider scaling down context sizes or limiting concurrent file processing. Final polish: Clean up the UI (styles, remove any placeholder text), add help tooltips or an introduction message from Astra explaining its capabilities to the user. Make sure all secrets and config are properly set for production (especially Google OAuth redirect URIs, etc.). At this stage, Astra 2.0 should be fully operational and tested â€“ ready for launch at Sunday 5pm PST.
[ ]
High
Steps 5-20 (all major features)
22
Optional Extensions & Future Work
(N/A â€“ ideas for improvement)
Beyond the weekend, consider adding: Advanced sentiment/voice analysis â€“ integrate an API or model to detect tone from voice more accurately (e.g., IBM Tone Analyzer or multi-modal emotion AI) for even richer emotional insight. Streaming and real-time conversation â€“ implement partial streaming of GPT responses (and ElevenLabs streaming if available) to make voice replies start faster, approaching real-time back-and-forth. Vision capabilities â€“ if Astra could access a camera or images, add an image recognition module to allow queries on visual data. Mobile app or UI â€“ wrap the project in a mobile-friendly app for on-the-go access to Astra. Collaboration between Astra instances â€“ build a secure mechanism for different Astra deployments to share knowledge selectively (perhaps a federation protocol or using Supabaseâ€™s shared tables concept). Plugin architecture â€“ allow Astra to use external tools or plugins (similar to ChatGPT plugins) to extend functionality (for example, a web browsing plugin to fetch live data, or integration with IoT devices). Further self-improvement â€“ incorporate a reward model so Astra knows which self-edits improved performance (closing the learning loop). Security enhancements â€“ conduct thorough security audit (the system deals with sensitive data like emails and can execute code, so locking down access and sandboxing is crucial). These extensions could make Astra even more powerful and autonomous. They are not required for the MVP launch but provide a roadmap for future versions.
[ ]
Low
System fully functional (core complete)
# ASTRA-2.0
